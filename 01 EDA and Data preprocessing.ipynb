{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862a3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math, os, re, time, random, string\n",
    "import numpy as np, pandas as pd, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "import wordcloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae70238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./raw_data/reddit_train.csv', encoding='latin-1')\n",
    "test = pd.read_csv('./raw_data/reddit_test.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146bf655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(df):\n",
    "    df['word count'] = df['BODY'].apply(lambda x: len(x.split())) #count number of words in text\n",
    "    df['unique word count'] = df['BODY'].apply(lambda x: len(set(x.split()))) #count unique words\n",
    "    df['char count'] = df['BODY'].apply(lambda x: len(x)) #count number of characters\n",
    "    df['url count'] = df['BODY'].apply(lambda x: len([i for i in str(x) if i == 'http'])) #count number of urls\n",
    "    df['stopword count'] = df['BODY'].apply(lambda x: len([i for i in x.lower().split() if i in wordcloud.STOPWORDS]))\n",
    "    df['punctuation count'] = df['BODY'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n",
    "    \n",
    "features(df)\n",
    "features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ba9649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'X', 'BODY', 'REMOVED', 'word count', 'unique word count',\n",
       "       'char count', 'url count', 'stopword count', 'punctuation count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3a123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square bracketsremove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')+['u', 'im', 'c']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text \n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.drop(['Unnamed: 0', 'X'], axis=1)\n",
    "    df['cleaned'] = df['BODY'].apply(clean_text)\n",
    "    df['stop_removed'] = df['cleaned'].apply(remove_stopwords)\n",
    "    df['stemmed_no_stop'] = df['stop_removed'].apply(stemm_text)\n",
    "    df['stemmed_with_stop'] = df['cleaned'].apply(stemm_text)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773824b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbda652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./raw_data/train_processed.csv', index=False)\n",
    "test.to_csv('./raw_data/test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe478c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BODY', 'REMOVED', 'word count', 'unique word count', 'char count',\n",
       "       'url count', 'stopword count', 'punctuation count', 'cleaned',\n",
       "       'stop_removed', 'stemmed_no_stop', 'stemmed_with_stop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac5c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
